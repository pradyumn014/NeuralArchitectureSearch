{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Some problems\n",
    "#### remove morph function from Network class to shift to hill climbing, also kernel_size and widening factor \n",
    "#### for deepen and widen should be parameters\n",
    "### check more conformity betweeen input adj_list and input 'int_to_node'\n",
    "### check compatibility of initial arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from graphviz import Digraph\n",
    "import random\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1077c2d70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error(Exception):\n",
    "    def __init__(self, expr = None, msg = None):\n",
    "        self.expr = expr\n",
    "        self.msg = msg\n",
    "class inputSmallerThanKernel(Error):\n",
    "    def __init__(self):\n",
    "        super(inputSmallerThanKernel, self).__init__()\n",
    "class nodeDoesNotExist(Error):\n",
    "    def __init__(self):\n",
    "        super(nodeDoesNotExist, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node(object):\n",
    "    nodes = []\n",
    "    node_type = 'simple'\n",
    "    def __init__(self, input_shape = (0, 0, 0), output_shape = (0, 0, 0)): # c, i1, i2\n",
    "        node.nodes.append(self)\n",
    "        self.no  = len(node.nodes)\n",
    "        self.in_adj = []\n",
    "        self.out_adj = []\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.compatible = True\n",
    "            \n",
    "    def node_alright(self, curr_node):\n",
    "        try:\n",
    "            assert(issubclass(type(curr_node), node))\n",
    "        except:\n",
    "            raise Error('Not a node' + str(type(curr_node)))\n",
    "# Put this section in graph class\n",
    "#         try:\n",
    "#             assert(curr_node in graph_nodes)\n",
    "#         except:\n",
    "#             raise nodeDoesNotExist\n",
    "    \n",
    "    def determine_compatibility(self):\n",
    "        for curr_node in self.in_adj:\n",
    "            curr = (curr_node.output_shape == self.input_shape)\n",
    "            self.compatible  = self.compatible and curr\n",
    "            \n",
    "        for curr_node in self.out_adj:\n",
    "            curr = (curr_node.input_shape == self.output_shape)\n",
    "            self.compatible = self.compatible and curr\n",
    "\n",
    "    def describe_adj_list(self, in_adj, out_adj):\n",
    "        assert isinstance(in_adj, list), 'in_adj must be a list'\n",
    "        assert isinstance(in_adj, list), 'out_adj must be a list'\n",
    "        for curr_node in in_adj + out_adj:\n",
    "            try:\n",
    "                self.node_alright(curr_node)\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "        self.in_adj = in_adj\n",
    "        self.out_adj = out_adj\n",
    "\n",
    "    def out_shape(self):\n",
    "        pass\n",
    "    \n",
    "    def remove(self):\n",
    "        ## needed in graph class\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.no) + \" \" + str(node.node_type) \n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convolution_block(nn.Module, node):\n",
    "    all_convs = []\n",
    "    node_type = 'conv'\n",
    "    def __init__(self, in_h, in_w, in_channels, out_channels, kernel_size, padding = 0, stride = 1):\n",
    "        try:\n",
    "            assert(min(in_h, in_w) +2*padding >= kernel_size)\n",
    "        except:\n",
    "            raise inputSmallerThanKernel\n",
    "        super(convolution_block, self).__init__()\n",
    "        node.__init__(self, (in_channels, in_h, in_w))\n",
    "        convolution_block.all_convs.append(self)\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels \n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride \n",
    "        self.padding = padding\n",
    "        self.output_shape = self.out_shape()\n",
    "        \n",
    "        # NN Layers\n",
    "        self.conv_layer = nn.Conv2d(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.padding)\n",
    "        self.batch_norm = nn.BatchNorm2d(self.out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    def out_shape(self):\n",
    "        c, h, w = self.input_shape\n",
    "        C = self.out_channels\n",
    "        H = (h + 2*self.padding - self.kernel_size)/self.stride + 1\n",
    "        W = (w + 2*self.padding - self.kernel_size)/self.stride + 1\n",
    "        return (C, H, W)\n",
    "    \n",
    "#     def determine_compatibility(self):\n",
    "#         super(convolution_block, self).determine_compatibility()\n",
    "#         self.compatible  = self.compatible and (len(self.in_adj) == 1)\n",
    "\n",
    "    def describe_adj_list(self, in_adj, out_adj):\n",
    "        super(convolution_block, self).describe_adj_list(in_adj, out_adj)\n",
    "        try:\n",
    "            assert(len(in_adj) == 1)\n",
    "        except:\n",
    "            print(in_adj)\n",
    "            raise Error('A convolution block can have only one in-edge')\n",
    "    \n",
    "    def remove(self): \n",
    "        ### Remove from all_convs list\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.no) + \" \" + str(convolution_block.node_type) \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class max_pool_node(nn.Module, node):\n",
    "    all_max_pools = []\n",
    "    node_type = 'max_pool'    \n",
    "    def __init__(self, in_h, in_w, in_channels, kernel_size, padding = 0, stride = 1):\n",
    "        try:\n",
    "            assert(min(in_h, in_w) +2*padding > kernel_size)\n",
    "        except:\n",
    "            raise inputSmallerThanKernel\n",
    "        super(max_pool_node, self).__init__()    \n",
    "        node.__init__(self, (in_channels, in_h, in_w))\n",
    "        max_pool_node.all_max_pools.append(self)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride \n",
    "        self.padding = padding\n",
    "        self.output_shape = self.out_shape()\n",
    "        \n",
    "        ## NN Layer\n",
    "        self.max_pool_layer = nn.MaxPool2d(self.kernel_size, self.stride, self.padding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.max_pool_layer(x)\n",
    "    \n",
    "    def out_shape(self):\n",
    "        c, h, w = self.input_shape\n",
    "        C = c\n",
    "        H = (h + 2*self.padding - self.kernel_size)/self.stride + 1\n",
    "        W = (w + 2*self.padding - self.kernel_size)/self.stride + 1\n",
    "        return (C, H, W)\n",
    "    \n",
    "#     def determine_compatibility(self):\n",
    "#         super(max_pool_node, self).determine_compatibility()\n",
    "#         self.compatible  = self.compatible and (len(self.in_adj) == 1)\n",
    "\n",
    "    def describe_adj_list(self, in_adj, out_adj):\n",
    "        super(max_pool_node, self).describe_adj_list(in_adj, out_adj)\n",
    "        try:\n",
    "            assert(len(in_adj) == 1)\n",
    "        except:\n",
    "            raise Error('A max-pool block can have only one in-edge')\n",
    "\n",
    "    def remove(self):\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.no) + \" \" + str(max_pool_node.node_type) \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Is is it required to be to a derived class of nn.Module ?\n",
    "class merge_node(node):\n",
    "    all_merge_nodes = []\n",
    "    node_type = 'merge'    \n",
    "    def __init__(self, parents, child):\n",
    "        super(merge_node, self).__init__()\n",
    "#         node.__init__(self)\n",
    "        try:\n",
    "            self.describe_adj_list(parents, child)\n",
    "        except Exception as e:\n",
    "            print e.expr\n",
    "            raise e\n",
    "        merge_node.all_merge_nodes.append(self)\n",
    "        \n",
    "    def describe_adj_list(self, in_adj, out_adj):\n",
    "        super(merge_node, self).describe_adj_list(in_adj, out_adj)\n",
    "        try:\n",
    "            assert(len(in_adj) == 2)\n",
    "        except:\n",
    "            raise Error('Parents must be exactly two')\n",
    "        \n",
    "class add_node(nn.Module, merge_node):\n",
    "    all_add_nodes = []\n",
    "    node_type = 'add'    \n",
    "    def __init__(self, parents, child):\n",
    "        super(add_node, self).__init__()\n",
    "        merge_node.__init__(self, parents, child)\n",
    "        add_node.all_add_nodes.append(self)\n",
    "        self.input_shape = self.in_adj[0].output_shape\n",
    "        self.output_shape = self.out_shape()\n",
    "                \n",
    "    ### Does it allow to input paramteters ?\n",
    "    def forward(self, x, y):\n",
    "        return x+y ### Check if their data strcutre type supports this addition\n",
    "        \n",
    "    def out_shape(self):\n",
    "        return self.input_shape\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.no) + \" \" + str(add_node.node_type) \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "class concat_node(merge_node):\n",
    "    # Define Later\n",
    "    pass\n",
    "\n",
    "class convex_merge_node(merge_node):\n",
    "    # Define Later\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module, object):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.adj_mat = {}\n",
    "        self.adj_list = {}\n",
    "        self.nodes = []\n",
    "        self.int_to_node = {}\n",
    "        self.node_to_int = {}\n",
    "        self.conv_blocks = []\n",
    "        self.max_pool_blocks = [] # Change naming conv maybe ?\n",
    "        self.topsort = []\n",
    "        self.rank_in_topsort = {}\n",
    "        self.max_no = 0\n",
    "        \n",
    "    def __init__(self, adj_list, int_to_node):\n",
    "        super(Network, self).__init__()\n",
    "        assert isinstance(int_to_node, dict), 'int_to_node must be a dictionary'\n",
    "        for _, cnode in int_to_node.items():\n",
    "            assert isinstance(cnode, node), 'mapping in int_to_node should be to a node'\n",
    "        \n",
    "        assert isinstance(adj_list, dict), 'adj_list should be a dictionary'\n",
    "        assert(len(int_to_node) == len(adj_list))\n",
    "        for cnode, li in adj_list.items():\n",
    "            assert cnode in int_to_node, 'mismatch between int_to_node and adj_list'\n",
    "            try:\n",
    "                assert(isinstance(li, list))\n",
    "                assert(len(li) == 2)\n",
    "                assert(isinstance(li[0], list) and isinstance(li[1], list))\n",
    "            except:\n",
    "                raise Error('Each mapping in adj_list should be to a two-dim list')\n",
    "            for child_node in li[0]:\n",
    "                assert child_node in int_to_node, 'mismatch between int_to_node and adj_list'\n",
    "            for child_node in li[1]:\n",
    "                assert child_node in int_to_node, 'mismatch between int_to_node and adj_list'\n",
    "\n",
    "        self.adj_list = adj_list\n",
    "        self.adj_mat = self.get_adj_mat(self.adj_list)\n",
    "        self.nodes = int_to_node.keys()\n",
    "        self.int_to_node = int_to_node\n",
    "        self.node_to_int = self.get_node_to_int(self.int_to_node)\n",
    "        self.max_no = max(self.int_to_node)\n",
    "        self.conv_blocks, self.max_pool_blocks = self.get_conv_and_max_pool_blocks()\n",
    "        self.topsort = []\n",
    "        self.rank_in_topsort = {}\n",
    "        self.topsorting()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.topsorting() # though this is not required here\n",
    "        outputs = {}\n",
    "        outputs[self.topsort[0]] = self.int_to_node(self.topsort[0]).forward(x)\n",
    "        for ind in range(1, len(self.topsort)):\n",
    "            node_no = self.topsort[ind]\n",
    "            curr_node = self.int_to_node[node_no]\n",
    "            outputs[node_no] = curr_node.forward(*map(lambda x: outputs[x], self.in_adj[node_no][0]))\n",
    "        return outputs[self.topsort[-1]]\n",
    "    \n",
    "    def get_node_to_int(self, int_to_node):\n",
    "        node_to_int = {}\n",
    "        for no, cnode in int_to_node.items():\n",
    "            node_to_int[cnode] = no\n",
    "        return node_to_int\n",
    "\n",
    "        \n",
    "    def get_adj_mat(self, adj_list):\n",
    "        adj_mat = {}\n",
    "        nodes = adj_list.keys()\n",
    "        for x in nodes:\n",
    "            adj_mat[x] = {}\n",
    "            for y in nodes:\n",
    "                adj_mat[x][y] = 0\n",
    "        for cnode, li in adj_list.items():\n",
    "            for par in li[0]:\n",
    "                adj_mat[par][cnode] = 1\n",
    "            for child in li[1]:\n",
    "                adj_mat[cnode][child] = 1\n",
    "        return adj_mat\n",
    "    \n",
    "    def get_conv_and_max_pool_blocks(self):\n",
    "        conv_blocks = []\n",
    "        max_pool_blocks = []\n",
    "        for x in self.nodes:\n",
    "            if isinstance(self.int_to_node[x], convolution_block):\n",
    "                conv_blocks.append(x)\n",
    "            elif isinstance(self.int_to_node[x], max_pool_node):\n",
    "                max_pool_blocks.append(x)\n",
    "        return (conv_blocks, max_pool_blocks)\n",
    "    \n",
    "    def topsorting(self):\n",
    "        # level problem\n",
    "        topsort = []\n",
    "        import Queue\n",
    "        in_deg = {}\n",
    "        q = Queue.Queue()\n",
    "        for node in self.nodes:\n",
    "            val  = len(self.adj_list[node][0])\n",
    "#             val = len(self.int_to_node[node].in_adj)\n",
    "            if val == 0:\n",
    "                q.put(node)\n",
    "            in_deg[node] = val\n",
    "            \n",
    "        while not q.empty():\n",
    "            curr_node = q.get()\n",
    "            topsort.append(curr_node)\n",
    "            for child in self.adj_list[curr_node][1]:\n",
    "                in_deg[child] -= 1\n",
    "                if in_deg[child] == 0:\n",
    "                    q.put(child)\n",
    "        self.topsort = topsort\n",
    "        self.set_rank_in_topsort()\n",
    "    \n",
    "    def set_rank_in_topsort(self):\n",
    "        for ind, node_no in enumerate(self.topsort):\n",
    "            self.rank_in_topsort[node_no]  = ind\n",
    "    \n",
    "    def add_nodes_to_network(self, nodes):\n",
    "        ### loophole here, assumption is that all changed nodes are being provided to the function\n",
    "        ### for now, lets go on with it, but its an issue\n",
    "        for curr_node in nodes:\n",
    "            curr_node.determine_compatibility()\n",
    "            if not curr_node.compatible:\n",
    "                raise Error('Node is not compatible with the graph') \n",
    "        for curr_node in nodes:\n",
    "            if curr_node not in self.node_to_int:\n",
    "                self.max_no += 1\n",
    "                self.adj_mat[self.max_no] = {}\n",
    "                self.adj_list[self.max_no] = [[], []]\n",
    "                self.node_to_int[curr_node] = self.max_no\n",
    "                self.int_to_node[self.max_no] = curr_node\n",
    "                self.nodes.append(self.max_no)\n",
    "                if isinstance(curr_node, convolution_block):\n",
    "                    self.conv_blocks.append(self.max_no)\n",
    "                elif isinstance(curr_node, max_pool_node):\n",
    "                    self.max_pool_blocks.append(self.max_no)\n",
    "        for curr_node in nodes:\n",
    "            no = self.node_to_int[curr_node]\n",
    "            try:\n",
    "                self.adj_list[no] = [map(lambda x: self.node_to_int[x], curr_node.in_adj), map(lambda x: self.node_to_int[x], curr_node.out_adj)]\n",
    "            except:\n",
    "#                 print self.node_to_int\n",
    "#                 print curr_node.in_adj\n",
    "#                 print curr_node.out_adj\n",
    "                raise Exception\n",
    "            for par in self.adj_list[no][0]:\n",
    "                self.adj_mat[par][no] = 1\n",
    "            for child in self.adj_list[no][0]:\n",
    "                self.adj_mat[no][child] = 1\n",
    "        self.topsorting()\n",
    "            \n",
    "    def deepen_morph(self):\n",
    "        deepen_conv_block = self.int_to_node[random.choice(self.conv_blocks)]\n",
    "        kernel_size = random.choice([3, 5])\n",
    "        in_channels, in_h, in_w = deepen_conv_block.output_shape\n",
    "        out_channels = in_channels\n",
    "        identity_conv_block = convolution_block(in_h, in_w, in_channels, out_channels, kernel_size, (kernel_size-1)/2)\n",
    "        weights = identity_conv_block.conv_layer.weight.data\n",
    "        \n",
    "        # creating identity weights\n",
    "        for channel in range(out_channels):\n",
    "            for i in range(in_channels):\n",
    "                for j in range(kernel_size):\n",
    "                    for k in range(kernel_size):\n",
    "                        weights[channel][i][j][k] = int((channel == i) and (j == k) and j == (kernel_size)/2 )\n",
    "#         print 'weights of identity conv block', weights\n",
    "        \n",
    "        ## make connections \n",
    "        identity_conv_block.describe_adj_list([deepen_conv_block], deepen_conv_block.out_adj)\n",
    "        deepen_conv_block.describe_adj_list(deepen_conv_block.in_adj, [identity_conv_block])\n",
    "\n",
    "        #### later look at creating a function for singular change to in_adj or out_adj of nodes\n",
    "        for out_node in identity_conv_block.out_adj:\n",
    "            out_node_in_adj = [identity_conv_block if (x == deepen_conv_block) else x for x in out_node.in_adj ]\n",
    "            out_node.describe_adj_list(out_node_in_adj, out_node.out_adj)\n",
    "        \n",
    "        self.add_nodes_to_network([deepen_conv_block, identity_conv_block] + identity_conv_block.out_adj)\n",
    "    \n",
    "    \n",
    "    def widen_morph(self):\n",
    "        candidate_conv_blocks = []\n",
    "        for conv_block in self.conv_blocks:\n",
    "            isCandidate = bool(len(self.adj_list[conv_block][1]))\n",
    "            for child in self.adj_list[conv_block][1]:\n",
    "                isCandidate = isCandidate and isinstance(self.int_to_node[child], convolution_block)\n",
    "            if isCandidate:\n",
    "                candidate_conv_blocks.append(conv_block)\n",
    "        if len(candidate_conv_blocks) == 0:\n",
    "            return False\n",
    "\n",
    "        parent_block_no = random.choice(candidate_conv_blocks)\n",
    "        parent_block = self.int_to_node[parent_block_no]\n",
    "        widening_factor = random.choice([2, 4])\n",
    "        in_channels, in_h, in_w = parent_block.input_shape\n",
    "        out_channels = parent_block.out_channels\n",
    "        kernel_size = parent_block.kernel_size\n",
    "        padding = parent_block.padding\n",
    "        stride = parent_block.stride\n",
    "        widened_parent_block = convolution_block(in_h, in_w, in_channels, out_channels*widening_factor, kernel_size, padding, stride)\n",
    "        original_parent_weight = parent_block.conv_layer.weight.data\n",
    "        widened_parent_weight = widened_parent_block.conv_layer.weight.data\n",
    "        widened_parent_weight[:out_channels] = original_parent_weight\n",
    "        widened_parent_weight[out_channels:] = torch.zeros((out_channels*(widening_factor-1), in_channels, kernel_size, kernel_size))\n",
    "        parent_block = widened_parent_block\n",
    "#         self.int_to_node[parent_block_no]  = widened_parent_block\n",
    "#         del self.node_to_int[parent_block]\n",
    "#         self.node_to_int[widened_parent_block] = parent_block_no\n",
    "        parent_out_adj = []\n",
    "        for child in parent_block.out_adj:\n",
    "            child_no = self.node_to_int[child]\n",
    "            in_channels, in_h, in_w = child.input_shape\n",
    "            out_channels = child.out_channels\n",
    "            kernel_size = child.kernel_size \n",
    "            padding = child.padding\n",
    "            stride = child.stride\n",
    "            child_widened = convolution_block(in_h, in_w, in_channels*widening_factor, out_channels, kernel_size, padding, stride)\n",
    "            child_widened.conv_layer.weight.data[:, :in_channels, :, :] = child.conv_layer.weight.data\n",
    "#             child_widened.describe_adj_list([widened_parent_block if x == parent_block else x for x in child.in_adj], child.out_adj)\n",
    "#             self.int_to_node[child_no] = child_widened\n",
    "#             del self.node_to_int[child]\n",
    "#             self.node_to_int[child_widened] = child_no\n",
    "#             parent_out_adj.append(child_widened)\n",
    "#         widened_parent_block.describe_adj_list(parent_block.in_adj, parent_out_adj)\n",
    "    \n",
    "    def dfs(self, curr_node, visited, weight):\n",
    "        visited[curr_node] = weight\n",
    "        for child in self.adj_list[curr_node][1]:\n",
    "            if child not in visited:\n",
    "                kernel = 0\n",
    "                padding = 0\n",
    "                constant = 0\n",
    "                child_node = self.int_to_node[child]\n",
    "                ## make adjustments for concatenation\n",
    "                if isinstance(child_node, convolution_block) or isinstance(child_node, max_pool_node):\n",
    "                    kernel = child_node.kernel_size\n",
    "                    padding = child_node.padding\n",
    "                    constant = 1\n",
    "                self.dfs(child, visited, [weight[0]+kernel, weight[1]+padding, weight[2]+constant])\n",
    "        \n",
    "        \n",
    "    def get_descendant_vectors(self):\n",
    "        descs = {}\n",
    "        for curr_node in self.nodes:\n",
    "            visited = {}\n",
    "            self.dfs(curr_node, visited, [0, 0, 0])\n",
    "            del visited[curr_node] # remove root \n",
    "            descs[curr_node] = visited\n",
    "        return descs\n",
    "        \n",
    "    def skip_morph(self):\n",
    "        descs = self.get_descendant_vectors()\n",
    "        candidates = [(ans, des) for ans in descs for des in descs[ans] ]\n",
    "        no1, no2 = random.choice(candidates)\n",
    "        weight = descs[no1][no2]\n",
    "        #join outputs of node1 and node2 using a merge block\n",
    "        node_a = self.int_to_node[no1]\n",
    "        node_b = self.int_to_node[no2]\n",
    "        out_ch_1, out_h_1, out_w_1 = node_a.output_shape\n",
    "        out_ch_2, out_h_2, out_w_2 = node_b.output_shape\n",
    "#         print 'selected_nodes are ', no1, \"  \",no2\n",
    "#         print 'weight is   ', weight\n",
    "#         print(out_h_1, out_h_2, weight[0] - 2*weight[1] - weight[2])\n",
    "        assert(out_h_1 - out_h_2 == out_w_1 - out_w_2)\n",
    "        assert(out_h_1 - out_h_2 == weight[0] - 2*weight[1] - weight[2])\n",
    "        if weight[2] & 1 == 0:\n",
    "            weight[2] += 1\n",
    "            weight[0] += 1\n",
    "        weight[1] += (weight[2])/2\n",
    "        weight[2] -= 2*(weight[2]/2)\n",
    "        kernel_size = weight[0]\n",
    "        padding = weight[1]\n",
    "        stride = 1\n",
    "        new_conv = convolution_block(out_h_1, out_w_1, out_ch_1, out_ch_2, kernel_size, padding, stride)\n",
    "        new_add = add_node([new_conv, node_b], node_b.out_adj)\n",
    "        new_conv.describe_adj_list([node_a], [new_add])\n",
    "        new_conv.conv_layer.weight.data = torch.zeros(new_conv.conv_layer.weight.data.shape)\n",
    "        node_a.describe_adj_list(node_a.in_adj, node_a.out_adj+[new_conv])\n",
    "        for child_node in node_b.out_adj:\n",
    "            child_node.describe_adj_list([new_add if x==node_b else x for x in child_node.in_adj], child_node.out_adj)\n",
    "        node_b.describe_adj_list(node_b.in_adj, [new_add])\n",
    "        self.add_nodes_to_network([node_a, node_b, new_conv, new_add] + new_add.out_adj)\n",
    "        \n",
    "        \n",
    "        ###\n",
    "    \n",
    "    def visualize(self):\n",
    "        graph = Digraph('./images/arch', './images/arch.gv')\n",
    "        for no, curr_node in self.int_to_node.items():\n",
    "#             graph.node(str(no), str(type(curr_node)).split('__main__.')[1])\n",
    "            graph.node(str(no), str(self.node_to_int[curr_node]) + \" :: \" + repr(curr_node)[:200])\n",
    "        for no, li in self.adj_list.items():\n",
    "            for ch in li[1]:\n",
    "                graph.edge(str(no), str(ch))\n",
    "        graph.view()\n",
    "#         x = IFrame(\"./images/archgv.pdf\", width=600, height=300)\n",
    "#         print x\n",
    "    \n",
    "    def describe(self):\n",
    "        print 'Nodes: ', self.nodes\n",
    "        print 'Conv_blocks', self.conv_blocks\n",
    "        print 'Max_pool_blocks', self.max_pool_blocks\n",
    "        print 'Adj_list', self.adj_list\n",
    "        print 'Adj_mat', self.adj_mat\n",
    "        print 'int_to_node', self.int_to_node\n",
    "        print 'node_to_int', self.node_to_int\n",
    "        print 'Toposort', self.topsort\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():    \n",
    "    dummy = node((5, 5, 5), (5, 5, 5))\n",
    "    n1 = convolution_block(5, 5, 5, 4, 3)\n",
    "    n2 = convolution_block(3, 3, 4, 3, 2)\n",
    "    dummy.describe_adj_list([], [n1])\n",
    "    n1.describe_adj_list([dummy], [n2])\n",
    "    n2.describe_adj_list([n1], [])\n",
    "    net = Network({0:[[], [1]], 1:[[0], [2]], 2: [[1], []]}, {0: dummy, 1:n1, 2:n2})\n",
    "    net.describe() \n",
    "    net.visualize()\n",
    "    for nan in net.nodes:\n",
    "        print net.int_to_node[nan].input_shape, ' -> ', nan, '->', net.int_to_node[nan].output_shape \n",
    "    for i in range(20):\n",
    "        net.skip_morph()\n",
    "        net.deepen_morph()\n",
    "        net.widen_morph()\n",
    "    net.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:  [0, 1, 2]\n",
      "Conv_blocks [1, 2]\n",
      "Max_pool_blocks []\n",
      "Adj_list {0: [[], [1]], 1: [[0], [2]], 2: [[1], []]}\n",
      "Adj_mat {0: {0: 0, 1: 1, 2: 0}, 1: {0: 0, 1: 0, 2: 1}, 2: {0: 0, 1: 0, 2: 0}}\n",
      "int_to_node {0: 167 simple, 1: 168 conv, 2: 169 conv}\n",
      "node_to_int {168 conv: 1, 167 simple: 0, 169 conv: 2}\n",
      "Toposort [0, 1, 2]\n",
      "(5, 5, 5)  ->  0 -> (5, 5, 5)\n",
      "(5, 5, 5)  ->  1 -> (4, 3, 3)\n",
      "(4, 3, 3)  ->  2 -> (3, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='./cifardata', train=True, download=True, transform=transform)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root='./cifardata', train=False, download=True, transform=transform)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "#Training\n",
    "n_training_samples = 20000\n",
    "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
    "\n",
    "#Validation\n",
    "n_val_samples = 5000\n",
    "val_sampler = SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype=np.int64))\n",
    "\n",
    "#Test\n",
    "n_test_samples = 5000\n",
    "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_loader(batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n",
    "    return(train_loader)\n",
    "\n",
    "#Test and validation loaders have constant batch sizes, so we can define them directly\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=4, sampler=test_sampler, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(train_set, batch_size=128, sampler=val_sampler, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def createLossAndOptimizer(net, learning_rate=0.001):\n",
    "    \n",
    "    #Loss function\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return(loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    #Print all of the hyperparameters of the training iteration:\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    #Get training data\n",
    "    train_loader = get_train_loader(batch_size)\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    #Create our loss and optimizer functions\n",
    "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            #Get inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            #Wrap them in a Variable object\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs, labels)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            running_loss += loss_size.data[0]\n",
    "            total_train_loss += loss_size.data[0]\n",
    "            \n",
    "            #Print every 10th batch of an epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            \n",
    "            #Wrap tensors in Variables\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss_size.data[0]\n",
    "            \n",
    "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_train():    \n",
    "    dummy = node((5, 5, 5), (5, 5, 5))\n",
    "    n1 = convolution_block(5, 5, 5, 4, 3)\n",
    "    n2 = convolution_block(3, 3, 4, 3, 2)\n",
    "    dummy.describe_adj_list([], [n1])\n",
    "    n1.describe_adj_list([dummy], [n2])\n",
    "    n2.describe_adj_list([n1], [])\n",
    "    net = Network({0:[[], [1]], 1:[[0], [2]], 2: [[1], []]}, {0: dummy, 1:n1, 2:n2})\n",
    "#     net.describe() \n",
    "    net.visualize()\n",
    "#     for nan in net.nodes:\n",
    "#         print net.int_to_node[nan].input_shape, ' -> ', nan, '->', net.int_to_node[nan].output_shape \n",
    "#     for i in range(20):\n",
    "#         net.skip_morph()\n",
    "#         net.deepen_morph()\n",
    "#         net.widen_morph()\n",
    "#     net.visualize()\n",
    "    trainNet(net, batch_size=32, n_epochs=5, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== HYPERPARAMETERS =====\n",
      "('batch_size=', 32)\n",
      "('epochs=', 5)\n",
      "('learning_rate=', 0.0001)\n",
      "==============================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-503441be3165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdummy_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-21441b240566>\u001b[0m in \u001b[0;36mdummy_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#         net.widen_morph()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#     net.visualize()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrainNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-d47d39e8776f>\u001b[0m in \u001b[0;36mtrainNet\u001b[0;34m(net, batch_size, n_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#Create our loss and optimizer functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateLossAndOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#Time for printing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-0cbf35c172ee>\u001b[0m in \u001b[0;36mcreateLossAndOptimizer\u001b[0;34m(net, learning_rate)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torch/optim/adam.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[1;32m     39\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[1;32m     40\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torch/optim/optimizer.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "dummy_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hill_climbing:\n",
    "    def __init__(X, Y, iterations = 100, init_network = None):\n",
    "        ## If init network is not provided, provide facility to begin with random network\n",
    "        # X and Y are data and labels\n",
    "        self.iterations = iterations \n",
    "        if init_network:\n",
    "            self.net = init_network\n",
    "        else:\n",
    "            self.net = random_network()\n",
    "    \n",
    "    def plot_loss_graph(self):\n",
    "        pass\n",
    "    \n",
    "    def visualize_tree_of_networks(self):\n",
    "        pass\n",
    "    \n",
    "    def morphism(self):\n",
    "        import random\n",
    "        actions = {'deepen': self.net.deepen_morph, \n",
    "                   'widen': self.net.widen_morph, \n",
    "                   'skip-connection': self.net.skip_morph }\n",
    "        choice = random.choice(actions)\n",
    "        actions[choice]()\n",
    "        \n",
    "    def start(self):\n",
    "        for iteration in range(iterations):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
